# Quantization Fundamentals with Hugging Face (Deeplearning.ai course)
Ref: https://learn.deeplearning.ai/courses/quantization-fundamentals

## Lesson 3: Loading ML Models with Different Data Type 

Watch `Loading models by data type`, and then answer these questions: 

### Model casting 
1. What does the `print_param_dtype()` function do? 

   [Your answer here]
   
2. Show the code that can be used to **cast** the model parameters to bfloat16 data type. 

   [Your answer here]

3. What do we mean by performing an **inference** of a model. 

   [Your answer here]

4. What is `deepcopy` used in this video?

   [Your answer here]

5. The `model_bf16` uses lesser precision than the FP32 `model`. In the video, how do they compare the impact of this low precision on the model performance? Is the performance degradation significant?

   [Your answer here]

### Loading model in low precision 

6. Name the Pytorch method that can be used to get the memory footprint of a model. 

   [Your answer here]

7. How is loading the model at low precision better than downcasting the weights of a loaded model?

   [Your answer here]

### Linear quantisation

8. How is linear quantisation different from lower precision floating point representation?

   [Your answer here]

9. How does linear quantization lead to lower memory footprint?

   [Your answer here]


   
